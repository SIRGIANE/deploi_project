"""
Script d'Ã©valuation des modÃ¨les mÃ©tÃ©orologiques entraÃ®nÃ©s
Charge les modÃ¨les depuis MLflow et gÃ©nÃ¨re des rapports dÃ©taillÃ©s
"""

import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from config import Config
except ImportError:
    from src.config import Config

class ModelEvaluator:
    """Ã‰valuateur de modÃ¨les mÃ©tÃ©orologiques"""
    
    def __init__(self, mlflow_uri: str = "file:./mlruns"):
        self.mlflow_uri = mlflow_uri
        mlflow.set_tracking_uri(mlflow_uri)
        
        # RÃ©pertoires de sortie
        self.reports_dir = Path("reports")
        self.reports_dir.mkdir(exist_ok=True)
        
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
    def load_latest_training_results(self) -> Optional[Dict[str, Any]]:
        """Charge les derniers rÃ©sultats d'entraÃ®nement"""
        results_file = self.results_dir / "weather_training_results.json"
        
        if not results_file.exists():
            logger.error(f"âŒ Fichier de rÃ©sultats introuvable: {results_file}")
            return None
            
        try:
            with open(results_file, 'r') as f:
                results = json.load(f)
            logger.info(f"âœ… RÃ©sultats d'entraÃ®nement chargÃ©s: {results_file}")
            return results
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement des rÃ©sultats: {e}")
            return None
    
    def evaluate_latest_models(self) -> Dict[str, Any]:
        """Ã‰value les derniers modÃ¨les entraÃ®nÃ©s"""
        logger.info("ğŸ“Š Ã‰VALUATION DES MODÃˆLES MÃ‰TÃ‰O")
        logger.info("=" * 50)
        
        # Charger les rÃ©sultats d'entraÃ®nement
        training_results = self.load_latest_training_results()
        if not training_results:
            raise ValueError("Impossible de charger les rÃ©sultats d'entraÃ®nement")
        
        best_model_name = training_results['best_model']
        models_performance = training_results['models_performance']
        
        logger.info(f"ğŸ† Meilleur modÃ¨le identifiÃ©: {best_model_name}")
        
        # Ã‰valuation dÃ©taillÃ©e
        evaluation_results = {
            'evaluation_timestamp': datetime.now().isoformat(),
            'best_model': best_model_name,
            'performance_summary': self._generate_performance_summary(models_performance),
            'model_comparison': self._compare_models_detailed(models_performance),
            'deployment_ready': training_results.get('deployment_recommendation', {}).get('should_deploy', False),
            'data_quality_check': self._check_data_quality(training_results),
            'recommendations': self._generate_recommendations(training_results)
        }
        
        # GÃ©nÃ©ration des rapports
        self._generate_model_card(evaluation_results, training_results)
        self._generate_comparison_report(evaluation_results)
        
        # Sauvegarde des rÃ©sultats d'Ã©valuation
        eval_file = self.results_dir / "model_evaluation_results.json"
        with open(eval_file, 'w') as f:
            json.dump(evaluation_results, f, indent=2, default=str)
        
        logger.info(f"âœ… Ã‰valuation terminÃ©e. Rapport sauvÃ©: {eval_file}")
        return evaluation_results
    
    def _generate_performance_summary(self, models_performance: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
        """GÃ©nÃ¨re un rÃ©sumÃ© des performances"""
        summary = {}
        
        for model_name, metrics in models_performance.items():
            summary[model_name] = {
                'test_rmse': round(metrics['avg_test_rmse'], 4),
                'test_r2': round(metrics['avg_test_r2'], 4),
                'test_mae': round(metrics['avg_test_mae'], 4),
                'performance_grade': self._grade_performance(metrics['avg_test_r2'])
            }
        
        return summary
    
    def _grade_performance(self, r2_score: float) -> str:
        """Attribue une note de performance basÃ©e sur le RÂ²"""
        if r2_score >= 0.95:
            return "A+ (Excellent)"
        elif r2_score >= 0.90:
            return "A (TrÃ¨s bon)"
        elif r2_score >= 0.80:
            return "B (Bon)"
        elif r2_score >= 0.70:
            return "C (Acceptable)"
        else:
            return "D (Insuffisant)"
    
    def _compare_models_detailed(self, models_performance: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
        """Comparaison dÃ©taillÃ©e des modÃ¨les"""
        comparison = {
            'ranking_by_rmse': [],
            'ranking_by_r2': [],
            'performance_gaps': {}
        }
        
        # Classement par RMSE (plus bas = meilleur)
        rmse_ranking = sorted(models_performance.items(), key=lambda x: x[1]['avg_test_rmse'])
        comparison['ranking_by_rmse'] = [
            {'model': name, 'rmse': metrics['avg_test_rmse']} 
            for name, metrics in rmse_ranking
        ]
        
        # Classement par RÂ² (plus haut = meilleur)
        r2_ranking = sorted(models_performance.items(), key=lambda x: x[1]['avg_test_r2'], reverse=True)
        comparison['ranking_by_r2'] = [
            {'model': name, 'r2': metrics['avg_test_r2']} 
            for name, metrics in r2_ranking
        ]
        
        # Calcul des Ã©carts de performance
        best_rmse = rmse_ranking[0][1]['avg_test_rmse']
        best_r2 = r2_ranking[0][1]['avg_test_r2']
        
        for model_name, metrics in models_performance.items():
            rmse_gap = ((metrics['avg_test_rmse'] - best_rmse) / best_rmse * 100) if best_rmse > 0 else 0
            r2_gap = ((best_r2 - metrics['avg_test_r2']) / best_r2 * 100) if best_r2 > 0 else 0
            
            comparison['performance_gaps'][model_name] = {
                'rmse_gap_percent': round(rmse_gap, 2),
                'r2_gap_percent': round(r2_gap, 2)
            }
        
        return comparison
    
    def _check_data_quality(self, training_results: Dict[str, Any]) -> Dict[str, Any]:
        """VÃ©rification de la qualitÃ© des donnÃ©es"""
        data_prep = training_results.get('data_preparation', {})
        
        quality_check = {
            'total_samples': data_prep.get('train_samples', 0) + data_prep.get('test_samples', 0),
            'train_test_ratio': round(data_prep.get('train_samples', 0) / data_prep.get('test_samples', 1), 2),
            'feature_count': data_prep.get('feature_count', 0),
            'target_variables': data_prep.get('target_variables', []),
            'data_quality_score': 'Good'  # SimplifiÃ©e pour cet exemple
        }
        
        # Ã‰valuation de la qualitÃ©
        if quality_check['total_samples'] < 1000:
            quality_check['data_quality_score'] = 'Limited'
        elif quality_check['train_test_ratio'] < 3 or quality_check['train_test_ratio'] > 5:
            quality_check['data_quality_score'] = 'Fair'
        
        return quality_check
    
    def _generate_recommendations(self, training_results: Dict[str, Any]) -> List[str]:
        """GÃ©nÃ¨re des recommandations basÃ©es sur les rÃ©sultats"""
        recommendations = []
        
        best_model = training_results['best_model']
        best_metrics = training_results['models_performance'][best_model]
        deployment_decision = training_results.get('deployment_recommendation', {})
        
        # Recommandations basÃ©es sur les performances
        if best_metrics['avg_test_r2'] >= 0.95:
            recommendations.append("ğŸ† Excellentes performances - ModÃ¨le prÃªt pour la production")
        elif best_metrics['avg_test_r2'] >= 0.90:
            recommendations.append("âœ… Bonnes performances - DÃ©ploiement recommandÃ©")
        else:
            recommendations.append("âš ï¸ Performances limitÃ©es - ConsidÃ©rer plus de donnÃ©es ou features")
        
        # Recommandations basÃ©es sur le dÃ©ploiement
        if deployment_decision.get('should_deploy', False):
            recommendations.append("ğŸš€ DÃ©ploiement automatique approuvÃ©")
        else:
            reasons = deployment_decision.get('reasons', [])
            if reasons:
                recommendations.append(f"â¸ï¸ DÃ©ploiement en attente: {'; '.join(reasons)}")
        
        # Recommandations techniques
        if best_model == 'LinearRegression':
            recommendations.append("ğŸ’¡ Relation linÃ©aire dÃ©tectÃ©e - ModÃ¨le simple mais efficace")
        elif best_model in ['RandomForest', 'GradientBoosting']:
            recommendations.append("ğŸŒ² ModÃ¨le complexe sÃ©lectionnÃ© - Surveiller le surapprentissage")
        
        return recommendations
    
    def _generate_model_card(self, evaluation_results: Dict[str, Any], training_results: Dict[str, Any]) -> None:
        """GÃ©nÃ¨re une carte de modÃ¨le dÃ©taillÃ©e"""
        model_card_path = self.reports_dir / "model_card.md"
        
        best_model = evaluation_results['best_model']
        best_metrics = evaluation_results['performance_summary'][best_model]
        
        model_card_content = f"""# Model Card - Climate MLOps

## Informations gÃ©nÃ©rales
- **ModÃ¨le**: {best_model}
- **Version**: {datetime.now().strftime('%Y.%m.%d')}
- **Date d'Ã©valuation**: {evaluation_results['evaluation_timestamp']}
- **Statut**: {'âœ… PrÃªt pour dÃ©ploiement' if evaluation_results['deployment_ready'] else 'â¸ï¸ En attente'}

## Performances
- **RMSE Test**: {best_metrics['test_rmse']}Â°C
- **RÂ² Test**: {best_metrics['test_r2']} ({best_metrics['performance_grade']})
- **MAE Test**: {best_metrics['test_mae']}Â°C

## Dataset
- **Source**: {training_results['data_preparation']['dataset']}
- **Ã‰chantillons total**: {evaluation_results['data_quality_check']['total_samples']}
- **Variables cibles**: {', '.join(training_results['data_preparation']['target_variables'])}
- **Features**: {evaluation_results['data_quality_check']['feature_count']}

## Comparaison des modÃ¨les
"""
        
        for model_name, summary in evaluation_results['performance_summary'].items():
            indicator = "ğŸ†" if model_name == best_model else "  "
            model_card_content += f"- {indicator} **{model_name}**: RMSE={summary['test_rmse']}, RÂ²={summary['test_r2']} ({summary['performance_grade']})\n"
        
        model_card_content += f"""
## Recommandations
"""
        for rec in evaluation_results['recommendations']:
            model_card_content += f"- {rec}\n"
        
        model_card_content += f"""
## MÃ©triques techniques
- **URI MLflow**: {training_results.get('mlflow_uri', 'N/A')}
- **ExpÃ©rience**: {training_results.get('mlflow_experiment', 'N/A')}
- **MÃ©thode de sÃ©lection**: {training_results.get('selection_method', 'N/A')}

*GÃ©nÃ©rÃ© automatiquement par Climate MLOps Pipeline*
"""
        
        with open(model_card_path, 'w') as f:
            f.write(model_card_content)
        
        logger.info(f"ğŸ“„ Carte de modÃ¨le gÃ©nÃ©rÃ©e: {model_card_path}")
    
    def _generate_comparison_report(self, evaluation_results: Dict[str, Any]) -> None:
        """GÃ©nÃ¨re un rapport de comparaison JSON pour les artefacts"""
        comparison_path = Path("model_metrics_comparison.json")
        
        comparison_data = {
            'evaluation_timestamp': evaluation_results['evaluation_timestamp'],
            'best_model': evaluation_results['best_model'],
            'models_ranking': evaluation_results['model_comparison']['ranking_by_rmse'],
            'performance_summary': evaluation_results['performance_summary'],
            'deployment_recommendation': evaluation_results['deployment_ready']
        }
        
        with open(comparison_path, 'w') as f:
            json.dump(comparison_data, f, indent=2)
        
        logger.info(f"ğŸ“Š Rapport de comparaison gÃ©nÃ©rÃ©: {comparison_path}")

def main():
    """Fonction principale d'Ã©valuation"""
    try:
        evaluator = ModelEvaluator()
        evaluation_results = evaluator.evaluate_latest_models()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Ã‰VALUATION DES MODÃˆLES TERMINÃ‰E")
        print("=" * 60)
        print(f"ğŸ† Meilleur modÃ¨le: {evaluation_results['best_model']}")
        print(f"ğŸš€ PrÃªt pour dÃ©ploiement: {'Oui' if evaluation_results['deployment_ready'] else 'Non'}")
        print(f"ğŸ“„ Rapports gÃ©nÃ©rÃ©s dans: ./reports/")
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'Ã©valuation: {e}")
        raise

if __name__ == "__main__":
    main()