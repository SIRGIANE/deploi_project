# üîß HYPERPARAM√àTRES D√âTAILL√âS - CLIMATE MLOPS

## üìã **R√âSUM√â EX√âCUTIF**

**Projet**: Climate MLOps - Pr√©diction M√©t√©o Marrakech  
**Dataset**: Donn√©es historiques 2018-2023 (2193 jours)  
**Mod√®le s√©lectionn√©**: **LinearRegression** (RMSE: 0.966¬∞C, R¬≤: 95.9%)  
**Date d'entra√Ænement**: 2025-12-13 18:56:27  

---

## üèÜ **1. LINEAR REGRESSION (MOD√àLE S√âLECTIONN√â)**

### Hyperparam√®tres
```python
{
    "fit_intercept": True,      # Calcul de l'ordonn√©e √† l'origine
    "copy_X": True,             # Copie des donn√©es d'entr√©e
    "n_jobs": None,             # Pas de parall√©lisation
    "positive": False           # Coefficients peuvent √™tre n√©gatifs
}
```

### D√©tail par cible
- `temperature_2m_mean`: RMSE=0.989¬∞C, R¬≤=97.9%
- `temperature_2m_min`: RMSE=1.908¬∞C, R¬≤=89.8% 
- `temperature_2m_max`: RMSE=0.000¬∞C, R¬≤=100% (relation parfaite)

---

## üå≤ **2. RANDOM FOREST (OPTIMIS√â OPTUNA)**

### Hyperparam√®tres optimis√©s
```python
{
    "n_estimators": 378,        # Nombre d'arbres dans la for√™t
    "max_depth": 25,            # Profondeur maximale des arbres
    "min_samples_split": 7,     # √âchantillons minimum pour diviser un n≈ìud
    "min_samples_leaf": 1,      # √âchantillons minimum dans une feuille
    "random_state": 42,         # Graine pour reproductibilit√©
    "max_features": "sqrt",     # ‚àö(n_features) par arbre (d√©faut)
    "bootstrap": True,          # √âchantillonnage avec remise
    "oob_score": False,         # Score out-of-bag d√©sactiv√©
    "n_jobs": -1,              # Utilisation de tous les CPU
    "criterion": "squared_error" # Crit√®re de division
}
```

### Espace de recherche Optuna
```python
OPTUNA_SEARCH_SPACE = {
    "n_estimators": (50, 500),     # 50 √† 500 arbres
    "max_depth": (5, 30),          # Profondeur 5 √† 30
    "min_samples_split": (2, 20),  # 2 √† 20 √©chantillons
    "min_samples_leaf": (1, 10)    # 1 √† 10 √©chantillons par feuille
}
```

### Configuration Optuna
- **M√©thode**: TPE (Tree-structured Parzen Estimator)
- **Nombre d'essais**: 20 trials
- **M√©trique d'optimisation**: RMSE (minimisation)
- **Timeout**: Aucun



---

## üìà **3. GRADIENT BOOSTING**

### Hyperparam√®tres
```python
{
    "n_estimators": 150,            # Nombre d'estimateurs de boosting
    "learning_rate": 0.1,           # Taux d'apprentissage (shrinkage)
    "max_depth": 6,                 # Profondeur maximale des arbres
    "min_samples_split": 2,         # √âchantillons minimum pour diviser
    "min_samples_leaf": 1,          # √âchantillons minimum par feuille
    "subsample": 1.0,               # Fraction d'√©chantillons utilis√©s
    "max_features": None,           # Toutes les features utilis√©es
    "random_state": 42,             # Graine al√©atoire
    "loss": "squared_error",        # Fonction de perte
    "criterion": "friedman_mse",    # Crit√®re de qualit√© de division
    "init": None,                   # Estimateur initial par d√©faut
    "alpha": 0.9,                   # Quantile pour perte Huber/quantile
    "verbose": 0,                   # Pas d'affichage du progr√®s
    "warm_start": False,            # Pas de r√©utilisation de solution
    "validation_fraction": 0.1,     # Fraction pour validation early stopping
    "n_iter_no_change": None,       # Pas d'early stopping
    "tol": 1e-4                     # Tol√©rance pour early stopping
}
```

### Multi-Output Configuration
```python
# Encapsul√© dans MultiOutputRegressor pour 3 cibles
MultiOutputRegressor(
    estimator=GradientBoostingRegressor(**params),
    n_jobs=None
)
```


---

## üìä **CONFIGURATION DES DONN√âES**

### Dataset
```python
{
    "source": "Marrakech Weather Dataset 2018-2023",
    "total_samples": 2193,
    "train_samples": 1754,         # 80% des donn√©es
    "test_samples": 439,           # 20% des donn√©es
    "train_test_split": 0.8,
    "split_method": "temporal"      # Division chronologique
}
```

### Variables cibles
```python
TARGET_VARIABLES = [
    "temperature_2m_mean",   # Temp√©rature moyenne (¬∞C)
    "temperature_2m_min",    # Temp√©rature minimale (¬∞C) 
    "temperature_2m_max"     # Temp√©rature maximale (¬∞C)
]
```

### Features (22 s√©lectionn√©es)
```python
SELECTED_FEATURES = [
    # Features temporelles
    "Year", "Month", "Quarter", "DayOfYear", "WeekOfYear",
    
    # Features cycliques
    "Month_sin", "Month_cos", "DayOfYear_sin", "DayOfYear_cos",
    
    # Features de lag (d√©calage temporel)
    "Temp_lag_1", "Temp_lag_3", "Temp_lag_7", "Temp_lag_14", "Temp_lag_30",
    
    # Moyennes mobiles
    "Temp_ma_3", "Temp_ma_7", "Temp_ma_14", "Temp_ma_30",
    
    # Features de tendance et volatilit√©
    "Temp_trend_30d", "Temp_volatility_7d",
    
    # Diff√©rences temporelles
    "Temp_diff_1d", "Temp_diff_7d"
]
```

### Configuration Feature Engineering
```python
FEATURE_CONFIG = {
    "LAG_PERIODS": [1, 3, 7, 14, 30],        # D√©calages en jours
    "MOVING_AVERAGE_WINDOWS": [3, 7, 14, 30], # Fen√™tres moyennes mobiles
    "TREND_WINDOW": 30,                       # Fen√™tre calcul tendance
    "VOLATILITY_WINDOW": 7,                   # Fen√™tre calcul volatilit√©
    "SCALING_METHOD": "StandardScaler"         # Normalisation Z-score
}
```

---

## ‚öôÔ∏è **S√âLECTION DE MOD√àLE AVANC√âE**

### Crit√®res de s√©lection
```python
MODEL_SELECTION_WEIGHTS = {
    "rmse": 0.4,    # 40% - Erreur quadratique (m√©trique principale)
    "r2": 0.3,      # 30% - Qualit√© d'ajustement
    "mae": 0.2,     # 20% - Erreur absolue moyenne
    "time": 0.1     # 10% - Vitesse d'entra√Ænement
}
```

### Scores composites obtenus
```python
COMPOSITE_SCORES = {
    "LinearRegression": 100.0,    # üèÜ GAGNANT
    "GradientBoosting": 54.5,
    "RandomForest": 0.0           # Performance la plus faible
}
```

### Crit√®res de d√©ploiement
```python
DEPLOYMENT_CRITERIA = {
    "min_r2_threshold": 0.7,           # R¬≤ minimum: 70%
    "min_rmse_improvement": 0.05,      # Am√©lioration RMSE: 5%
    "min_r2_improvement": 0.02,        # Am√©lioration R¬≤: 2%
    "min_data_points": 100,            # Points test minimum
    "max_training_time": 3600,         # Temps max: 1h
    "require_all_tests": True          # Tous crit√®res requis
}
```

### R√©sultat d√©ploiement
```python
DEPLOYMENT_DECISION = {
    "should_deploy": True,              # ‚úÖ D√âPLOYER
    "model_name": "LinearRegression",
    "reasons": [
        "‚úÖ R¬≤ satisfaisant: 95.9% >= 70%",
        "‚úÖ Premier mod√®le - pas de pr√©c√©dent",
        "‚úÖ Donn√©es suffisantes: 439 >= 100"
    ]
}
```

---

## üöÄ **CONFIGURATION MLFLOW**

### Tracking
```python
MLFLOW_CONFIG = {
    "tracking_uri": "file:./mlruns",
    "experiment_name": "training_20251213",
    "backend_store_uri": "./mlruns/mlflow.db",
    "default_artifact_root": "./mlruns"
}
```

### M√©triques track√©es
```python
TRACKED_METRICS = [
    "train_rmse", "test_rmse",         # Erreur quadratique
    "train_mae", "test_mae",           # Erreur absolue
    "train_r2", "test_r2",             # Coefficient d√©termination
    "training_time",                    # Temps d'entra√Ænement
    "composite_score"                   # Score de s√©lection
]
```

---

## üìà **PERFORMANCES COMPARATIVES**

| Mod√®le | RMSE Test | R¬≤ Test | MAE Test | Temps (s) | Score |
|--------|-----------|---------|----------|-----------|-------|
| **LinearRegression** | **0.966** | **95.9%** | **0.743** | **8.3** | **100** |
| GradientBoosting | 1.174 | 95.5% | 0.918 | 12.3 | 54.5 |
| RandomForest | 1.339 | 94.8% | 1.051 | 104.0 | 0.0 |

---

## üîß **ENVIRONNEMENT TECHNIQUE**

### Versions des librairies
```python
DEPENDENCIES = {
    "scikit-learn": ">=1.3.0",
    "optuna": ">=3.0.0",
    "mlflow": ">=2.0.0",
    "pandas": ">=2.0.0",
    "numpy": ">=1.24.0"
}
```

### Configuration syst√®me
```python
SYSTEM_CONFIG = {
    "python_version": "3.9+",
    "cpu_cores_used": "all (-1)",
    "memory_usage": "optimized",
    "random_seed": 42
}
```

---

## üí° **CONCLUSIONS**

1. **LinearRegression** domine gr√¢ce √† sa **simplicit√©** et **performance exceptionnelle**
2. La relation temp√©rature √† Marrakech est **largement lin√©aire**
3. Les mod√®les complexes (RF, GB) souffrent de **surapprentissage**
4. **Temps d'entra√Ænement** 12x plus rapide pour LinearRegression
5. **Recommandation**: D√©ployer LinearRegression en production

---

*G√©n√©r√© automatiquement le 2025-12-13 par Climate MLOps Pipeline*