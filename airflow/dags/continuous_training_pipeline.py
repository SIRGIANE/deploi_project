"""
ğŸŒ¦ï¸ Weather MLOps Continuous Training Pipeline
DAG pour l'entraÃ®nement continu des modÃ¨les mÃ©tÃ©orologiques avec Marrakech Weather Dataset
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
import os
import json
import logging
from pathlib import Path

# Configuration par dÃ©faut
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

# DÃ©finition du DAG
dag = DAG(
    'weather_continuous_training_pipeline',
    default_args=default_args,
    description='ğŸŒ¦ï¸ Pipeline de formation continue pour les modÃ¨les mÃ©tÃ©orologiques (Marrakech Weather Dataset)',
    schedule_interval='@weekly',  # ExÃ©cution hebdomadaire (tous les 7 jours)
    catchup=False,
    tags=['mlops', 'weather', 'marrakech', 'continuous-training'],
)

def ingest_new_data(**context):
    """TÃ©lÃ©chargement et ingestion des nouvelles donnÃ©es"""
    import sys
    sys.path.append('/workspace')
    from src.ingest_data import ingest_weather_data
    
    logging.info("ğŸš€ DÃ©marrage de l'ingestion automatique des donnÃ©es...")
    # Dans un cas rÃ©el, on passerait l'URL de l'API mÃ©tÃ©o ici
    ingest_weather_data()
    return "Ingestion terminÃ©e"

def check_data_drift(**context):
    """VÃ©rification du data drift (SimulÃ© car module optionnel supprimÃ©)"""
    logging.info("ğŸ” VÃ©rification du data drift mÃ©tÃ©orologique...")
    logging.info("âš ï¸ Module check_data_drift non prÃ©sent, passage de l'Ã©tape.")
    
    # Simulation: pas de drift dÃ©tectÃ© par dÃ©faut
    result = {
        'drift_detected': False,
        'drift_score': 0.0,
        'threshold': 0.3,
        'needs_retraining': True # Force retraining for demo purposes
    }
    
    logging.info(f"ğŸ“Š RÃ©sultats drift (simulÃ©): {result}")
    context['task_instance'].xcom_push(key='drift_results', value=result)
    return result['needs_retraining']

def load_marrakech_data(**context):
    """Chargement des donnÃ©es locales Marrakech"""
    import sys
    sys.path.append('/workspace')
    
    from src.marrakech_data_loader import MarrakechWeatherDataLoader
    
    logging.info("ğŸ“¥ Chargement des donnÃ©es Marrakech Weather depuis la DB...")
    
    # Le loader utilise maintenant la DB par dÃ©faut via les variables d'env
    loader = MarrakechWeatherDataLoader()
    df = loader.load_weather_data()
    
    logging.info(f"âœ… DonnÃ©es chargÃ©es: {len(df)} lignes")
    
    return {
        'source': 'database',
        'rows': len(df),
        'status': 'success'
    }

def validate_weather_data(**context):
    """Validation de la qualitÃ© des donnÃ©es mÃ©tÃ©orologiques"""
    import sys
    sys.path.append('/workspace')
    
    from src.data_pipeline import WeatherDataPipeline
    
    logging.info("âœ… Validation de la qualitÃ© des donnÃ©es mÃ©tÃ©orologiques...")
    
    pipeline = WeatherDataPipeline()
    weather_data = pipeline.load_and_prepare_data()
    is_valid, errors = pipeline.validate_data(weather_data)
    
    if not is_valid:
        raise Exception(f"âŒ Validation Ã©chouÃ©e: {errors}")
    
    validation_results = {
        'is_valid': is_valid,
        'data_shape': weather_data.shape,
        'columns_count': len(weather_data.columns)
    }
    
    logging.info(f"âœ… DonnÃ©es validÃ©es: {validation_results}")
    return validation_results

def run_dvc_pipeline(**context):
    """ExÃ©cution du pipeline DVC complet pour le projet mÃ©tÃ©o"""
    logging.info("ğŸ”„ ExÃ©cution du pipeline DVC mÃ©tÃ©o...")
    
    # Simulation ou commande rÃ©elle si DVC est configurÃ©
    # os.chdir('/workspace')
    # return {'pipeline_status': 'completed'}
    return {'pipeline_status': 'skipped (local mode)'}

def train_weather_models(**context):
    """EntraÃ®nement des modÃ¨les mÃ©tÃ©orologiques"""
    import sys
    sys.path.append('/workspace')
    
    from src.train_model import WeatherModelTrainer
    
    logging.info("ğŸ¤– EntraÃ®nement des modÃ¨les mÃ©tÃ©orologiques...")
    
    trainer = WeatherModelTrainer()
    
    # EntraÃ®nement complet
    results = trainer.run_full_training()
    
    context['task_instance'].xcom_push(key='training_results', value=results)
    
    return results

def evaluate_models(**context):
    """Ã‰valuation et comparaison des modÃ¨les"""
    import sys
    sys.path.append('/workspace')
    
    from src.model_comparison import evaluate_all_models, compare_with_production
    
    logging.info("ğŸ“Š Ã‰valuation des modÃ¨les mÃ©tÃ©o...")
    
    # RÃ©cupÃ©ration des rÃ©sultats d'entraÃ®nement
    training_results = context['task_instance'].xcom_pull(
        task_ids='train_weather_models',
        key='training_results'
    )
    
    # Ã‰valuation complÃ¨te
    evaluation_results = evaluate_all_models()
    
    # Comparaison avec le modÃ¨le en production
    comparison_results = compare_with_production(evaluation_results)
    
    should_promote = comparison_results.get('new_model_better', False)
    
    results = {
        'evaluation': evaluation_results,
        'comparison': comparison_results,
        'should_promote': should_promote,
        'best_model': comparison_results.get('best_model', training_results.get('best_model'))
    }
    
    context['task_instance'].xcom_push(key='model_results', value=results)
    
    return should_promote

def register_best_model(**context):
    """Enregistrement du meilleur modÃ¨le (SimulÃ©)"""
    logging.info("ğŸ† Enregistrement du modÃ¨le (Module register_model supprimÃ©)...")
    
    # RÃ©cupÃ©ration des rÃ©sultats
    model_results = context['task_instance'].xcom_pull(
        task_ids='evaluate_models',
        key='model_results'
    )
    
    if not model_results or not model_results.get('should_promote'):
        logging.info("ğŸš« Pas de nouveau modÃ¨le mÃ©tÃ©o Ã  promouvoir")
        return False
        
    best_model = model_results['best_model']
    logging.info(f"âœ… ModÃ¨le considÃ©rÃ© comme enregistrÃ©: {best_model.get('name', 'unknown')}")
    
    return {
        'model_registered': True,
        'model_version': 'v1_simulated',
        'model_name': best_model.get('name')
    }

def update_model_card(**context):
    """Mise Ã  jour de la documentation (SimulÃ©)"""
    logging.info("ğŸ“ Mise Ã  jour de la Model Card (Module generate_model_card supprimÃ©)...")
    return {'model_card_path': 'skipped'}

def send_notification(**context):
    """Envoi de notifications sur les rÃ©sultats du pipeline mÃ©tÃ©o"""
    
    # RÃ©cupÃ©ration des rÃ©sultats
    training_results = context['task_instance'].xcom_pull(
        task_ids='train_weather_models',
        key='training_results'
    ) or {}
    
    model_results = context['task_instance'].xcom_pull(
        task_ids='evaluate_models',
        key='model_results'
    ) or {}
    
    drift_results = context['task_instance'].xcom_pull(
        task_ids='check_data_drift',
        key='drift_results'
    ) or {}
    
    # Construction du message
    execution_date = context['execution_date'].strftime('%Y-%m-%d %H:%M:%S')
    
    best_model = training_results.get('best_model', 'N/A')
    # Gestion sÃ©curisÃ©e des mÃ©triques
    models_perf = training_results.get('models_performance', {})
    best_metrics = models_perf.get(best_model, {}) if isinstance(models_perf, dict) else {}
    
    message = f"""
    ğŸŒ¦ï¸ **Weather MLOps Pipeline - RÃ©sultats**
    
    ğŸ“… **Date d'exÃ©cution**: {execution_date}
    ğŸ“Š **Dataset**: Marrakech Weather
    
    ğŸ” **Data Drift**:
    - Score: {drift_results.get('drift_score', 'N/A')}
    
    ğŸ¤– **ModÃ¨les**:
    - Meilleur modÃ¨le: {best_model}
    - Test RMSE: {best_metrics.get('avg_test_rmse', 'N/A')}
    - Nouveau modÃ¨le promu: {'Oui' if model_results.get('should_promote', False) else 'Non'}
    
    âœ… **Pipeline exÃ©cutÃ© avec succÃ¨s**
    """
    
    logging.info(f"ğŸ“§ Notification: {message}")
    return {'notification_sent': True}

def re_run_data_pipeline(**context):
    """Re-exÃ©cution du pipeline de donnÃ©es aprÃ¨s ingestion pour rÃ©gÃ©nÃ©rer les features"""
    import sys
    sys.path.append('/workspace')
    
    from src.data_pipeline import WeatherDataPipeline
    
    logging.info("ğŸ”„ Re-exÃ©cution du pipeline de donnÃ©es aprÃ¨s ingestion...")
    
    pipeline = WeatherDataPipeline()
    results = pipeline.run_full_pipeline()
    
    logging.info("âœ… Pipeline de donnÃ©es re-exÃ©cutÃ© avec succÃ¨s")
    return {
        'pipeline_rerun': True,
        'new_data_shape': results['stats']['features_shape']
    }

# ========================
# DÃ‰FINITION DES TÃ‚CHES
# ========================

# 0. Ingestion des donnÃ©es (Nouvelle tÃ¢che)
ingest_data_task = PythonOperator(
    task_id='ingest_new_data',
    python_callable=ingest_new_data,
    dag=dag,
)

# 1. VÃ©rification du data drift
check_drift_task = PythonOperator(
    task_id='check_data_drift',
    python_callable=check_data_drift,
    dag=dag,
)

# 2. Chargement des donnÃ©es Marrakech
load_data_task = PythonOperator(
    task_id='load_marrakech_data',
    python_callable=load_marrakech_data,
    dag=dag,
)

# 3. Validation de la qualitÃ© des donnÃ©es
validate_data_task = PythonOperator(
    task_id='validate_weather_data',
    python_callable=validate_weather_data,
    dag=dag,
)

# 4. ExÃ©cution du pipeline DVC
run_pipeline_task = BashOperator(
    task_id='run_dvc_pipeline',
    bash_command='echo "DVC Pipeline skipped in local mode"',
    dag=dag,
)

# 5. EntraÃ®nement des modÃ¨les mÃ©tÃ©o
train_task = PythonOperator(
    task_id='train_weather_models',
    python_callable=train_weather_models,
    dag=dag,
)

# 6. Ã‰valuation des modÃ¨les
evaluate_task = PythonOperator(
    task_id='evaluate_models',
    python_callable=evaluate_models,
    dag=dag,
)

# 7. Enregistrement du meilleur modÃ¨le
register_model_task = PythonOperator(
    task_id='register_best_model',
    python_callable=register_best_model,
    dag=dag,
)

# 8. Mise Ã  jour de la documentation
update_docs_task = PythonOperator(
    task_id='update_model_card',
    python_callable=update_model_card,
    dag=dag,
)

# 9. Push vers GitHub
push_github_task = BashOperator(
    task_id='push_to_github',
    bash_command='echo "Git push skipped in local mode"',
    dag=dag,
)

# 10. Notifications
notify_task = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    dag=dag,
    trigger_rule='none_failed_min_one_success',
)

# 11. Re-exÃ©cution du pipeline de donnÃ©es aprÃ¨s ingestion
re_run_data_pipeline_task = PythonOperator(
    task_id='re_run_data_pipeline',
    python_callable=re_run_data_pipeline,
    dag=dag,
)

# ========================
# DÃ‰FINITION DES DÃ‰PENDANCES
# ========================

# Pipeline principal
# On commence par l'ingestion, puis on vÃ©rifie le drift sur les nouvelles donnÃ©es
ingest_data_task >> check_drift_task >> load_data_task >> validate_data_task
validate_data_task >> run_pipeline_task >> train_task
train_task >> evaluate_task >> register_model_task
register_model_task >> update_docs_task >> push_github_task >> notify_task

# Pipeline de notification toujours exÃ©cutÃ©
[check_drift_task, train_task, evaluate_task, register_model_task] >> notify_task

# Re-exÃ©cution du pipeline de donnÃ©es aprÃ¨s ingestion
ingest_data_task >> re_run_data_pipeline_task >> validate_data_task