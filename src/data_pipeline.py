"""
Pipeline de donnÃ©es pour le projet Climate MLOps avec Dataset Marrakech Weather
Pipeline en 3 Ã©tapes: raw â†’ processed â†’ features
"""

import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple, List, Optional

import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Import du loader Marrakech Weather
try:
    from src.marrakech_data_loader import MarrakechWeatherDataLoader
    from src.config import Config
except ImportError:
    from marrakech_data_loader import MarrakechWeatherDataLoader
    from config import Config

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Constants
DEFAULT_RAW_PATH = "data/raw"
DEFAULT_PROCESSED_PATH = "data/processed"
DEFAULT_FEATURES_PATH = "data/features"
DEFAULT_SPLIT_RATIO = 0.8
MIN_DATA_POINTS = 100  # RÃ©duit pour les donnÃ©es journaliÃ¨res de Marrakech

class WeatherDataPipeline:
    """Pipeline de traitement des donnÃ©es mÃ©tÃ©orologiques en 3 Ã©tapes"""
    
    def __init__(self, 
                 raw_path: str = DEFAULT_RAW_PATH,
                 processed_path: str = DEFAULT_PROCESSED_PATH,
                 features_path: str = DEFAULT_FEATURES_PATH,
                 data_file: str = None):
        self.raw_path = Path(raw_path)
        self.processed_path = Path(processed_path)
        self.features_path = Path(features_path)
        
        # CrÃ©ation des dossiers
        self.raw_path.mkdir(parents=True, exist_ok=True)
        self.processed_path.mkdir(parents=True, exist_ok=True)
        self.features_path.mkdir(parents=True, exist_ok=True)
        
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.is_fitted = False
        self._feature_columns: List[str] = []
        self._target_columns: List[str] = []
        
        # Utilisation du fichier de donnÃ©es Marrakech
        if data_file is None:
            data_file = Config.get_data_file_path()
        
        self.marrakech_loader = MarrakechWeatherDataLoader(str(data_file))
        
    # ============================================================================
    # Ã‰TAPE 1: CHARGEMENT DES DONNÃ‰ES LOCALES â†’ data/raw/
    # ============================================================================
    
    def step1_download_raw_data(self) -> pd.DataFrame:
        """
        Ã‰TAPE 1: Charge les donnÃ©es depuis le fichier local et les stocke dans data/raw/
        
        Returns:
            pd.DataFrame: DonnÃ©es brutes chargÃ©es
        """
        logger.info("=" * 70)
        logger.info("Ã‰TAPE 1: CHARGEMENT DES DONNÃ‰ES MÃ‰TÃ‰O DE MARRAKECH")
        logger.info("=" * 70)
        
        try:
            # Chargement depuis le fichier local
            weather_df = self.marrakech_loader.load_weather_data()
            
            # Sauvegarde dans data/raw/
            raw_file = self.raw_path / "weather_data_raw.csv"
            weather_df.to_csv(raw_file, index=False)
            logger.info(f"âœ… DonnÃ©es brutes sauvegardÃ©es: {raw_file}")
            logger.info(f"   ğŸ“Š Shape: {weather_df.shape}")
            logger.info(f"   ğŸ“‹ Colonnes: {list(weather_df.columns[:10])}...")
            
            return weather_df
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement: {e}")
            raise
    
    # ============================================================================
    # Ã‰TAPE 2: PREPROCESSING DES DONNÃ‰ES â†’ data/processed/
    # ============================================================================
    
    def step2_preprocess_data(self, df: pd.DataFrame = None) -> pd.DataFrame:
        """
        Ã‰TAPE 2: Applique le preprocessing et stocke dans data/processed/
        
        Args:
            df: DataFrame brut (ou charge depuis data/raw/ si None)
            
        Returns:
            pd.DataFrame: DonnÃ©es prÃ©processÃ©es
        """
        logger.info("=" * 70)
        logger.info("Ã‰TAPE 2: PREPROCESSING DES DONNÃ‰ES")
        logger.info("=" * 70)
        
        try:
            # Chargement depuis raw si nÃ©cessaire
            if df is None:
                raw_file = self.raw_path / "weather_data_raw.csv"
                if not raw_file.exists():
                    logger.warning("âš ï¸ Fichier raw non trouvÃ©, chargement...")
                    df = self.step1_download_raw_data()
                else:
                    logger.info(f"ğŸ“‚ Chargement depuis: {raw_file}")
                    df = pd.read_csv(raw_file)
            
            # PrÃ©processing
            df_processed = self.marrakech_loader.preprocess_weather_data(df)
            
            # Sauvegarde dans data/processed/
            processed_file = self.processed_path / "weather_data_processed.csv"
            df_processed.to_csv(processed_file, index=False)
            logger.info(f"âœ… DonnÃ©es prÃ©processÃ©es sauvegardÃ©es: {processed_file}")
            logger.info(f"   ğŸ“Š Shape: {df_processed.shape}")
            
            return df_processed
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du preprocessing: {e}")
            raise
    
    # ============================================================================
    # Ã‰TAPE 3: CRÃ‰ATION DES FEATURES â†’ data/features/
    # ============================================================================
    
    def step3_create_features(self, df: pd.DataFrame = None) -> pd.DataFrame:
        """
        Ã‰TAPE 3: CrÃ©e les features avancÃ©es et stocke dans data/features/
        
        Args:
            df: DataFrame prÃ©processÃ© (ou charge depuis data/processed/ si None)
            
        Returns:
            pd.DataFrame: DonnÃ©es avec features enrichies
        """
        logger.info("=" * 70)
        logger.info("Ã‰TAPE 3: CRÃ‰ATION DES FEATURES")
        logger.info("=" * 70)
        
        try:
            # Chargement depuis processed si nÃ©cessaire
            if df is None:
                processed_file = self.processed_path / "weather_data_processed.csv"
                if not processed_file.exists():
                    logger.warning("âš ï¸ Fichier processed non trouvÃ©, preprocessing...")
                    df = self.step2_preprocess_data()
                else:
                    logger.info(f"ğŸ“‚ Chargement depuis: {processed_file}")
                    df = pd.read_csv(processed_file)
                    # Reconversion de datetime si nÃ©cessaire
                    if 'datetime' in df.columns:
                        df['datetime'] = pd.to_datetime(df['datetime'])
            
            # CrÃ©ation des features
            df_features = self.marrakech_loader.create_weather_features(df)
            
            # Sauvegarde dans data/features/
            features_file = self.features_path / "weather_data_features.csv"
            df_features.to_csv(features_file, index=False)
            logger.info(f"âœ… Features sauvegardÃ©es: {features_file}")
            logger.info(f"   ğŸ“Š Shape: {df_features.shape}")
            
            return df_features
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la crÃ©ation des features: {e}")
            raise
    
    # ============================================================================
    # VALIDATION DES DONNÃ‰ES
    # ============================================================================
    
    def validate_data(self, df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """Validation de la qualitÃ© des donnÃ©es"""
        logger.info("ğŸ” Validation des donnÃ©es...")
        
        errors = []
        
        try:
            # VÃ©rification de la taille minimale
            if len(df) < MIN_DATA_POINTS:
                errors.append(f"DonnÃ©es insuffisantes: {len(df)} < {MIN_DATA_POINTS}")
            
            # VÃ©rification des colonnes numÃ©riques
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) < 5:
                errors.append(f"Trop peu de features numÃ©riques: {len(numeric_cols)}")
            
            # VÃ©rification du pourcentage de valeurs manquantes
            missing_pct = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
            if missing_pct > 0.3:
                errors.append(f"Taux Ã©levÃ© de valeurs manquantes: {missing_pct:.2%}")
            
        except Exception as e:
            errors.append(f"Erreur lors de la validation: {str(e)}")
        
        is_valid = len(errors) == 0
        if is_valid:
            logger.info("âœ… Validation des donnÃ©es rÃ©ussie")
        else:
            logger.error(f"âŒ Validation Ã©chouÃ©e: {errors}")
            
        return is_valid, errors
    
    # ============================================================================
    # PRÃ‰PARATION DES DONNÃ‰ES POUR ML
    # ============================================================================
    
    def _detect_targets(self, df: pd.DataFrame) -> List[str]:
        """DÃ©tecte plusieurs cibles mÃ©tÃ©o (temp, humiditÃ©, vent, prÃ©cipitations, pression)."""
        targets = []
        
        excluded_suffixes = ['_lag', '_ma', '_volatility', '_diff', '_trend', '_encoded', '_rolling', '_sin', '_cos']
        
        for col in df.columns:
            if any(col.endswith(suffix) or suffix in col for suffix in excluded_suffixes):
                continue
                
            col_lower = col.lower()
            
            if 'temp' in col_lower or 'temperature' in col_lower:
                if pd.api.types.is_numeric_dtype(df[col]):
                    targets.append(col)
                    continue
            
            if 'humid' in col_lower or 'humidity' in col_lower:
                if pd.api.types.is_numeric_dtype(df[col]):
                    targets.append(col)
                    continue
            
            if 'wind' in col_lower and ('speed' in col_lower or 'bearing' in col_lower):
                if pd.api.types.is_numeric_dtype(df[col]):
                    targets.append(col)
                    continue
            
            if 'pressure' in col_lower or 'press' in col_lower:
                if pd.api.types.is_numeric_dtype(df[col]):
                    targets.append(col)
                    continue
            
            if ('precip' in col_lower or 'rain' in col_lower) and '_encoded' not in col_lower:
                if pd.api.types.is_numeric_dtype(df[col]):
                    targets.append(col)
        
        return list(dict.fromkeys(targets))

    def prepare_ml_data(self, df: pd.DataFrame = None, 
                       split_ratio: float = DEFAULT_SPLIT_RATIO,
                       target_columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        PrÃ©paration des donnÃ©es pour l'entraÃ®nement ML
        Charge depuis data/features/ si df n'est pas fourni
        """
        logger.info("=" * 70)
        logger.info("PRÃ‰PARATION DES DONNÃ‰ES POUR ML")
        logger.info("=" * 70)
        
        try:
            # Chargement depuis features si nÃ©cessaire
            if df is None:
                features_file = self.features_path / "weather_data_features.csv"
                if not features_file.exists():
                    logger.warning("âš ï¸ Fichier features non trouvÃ©, crÃ©ation...")
                    df = self.step3_create_features()
                else:
                    logger.info(f"ğŸ“‚ Chargement depuis: {features_file}")
                    df = pd.read_csv(features_file)
            
            # SÃ©lection automatique des cibles mÃ©tÃ©o
            if target_columns is None:
                target_columns = self._detect_targets(df)
            if not target_columns:
                raise ValueError("Aucune variable mÃ©tÃ©o trouvÃ©e pour la prÃ©diction")
                
            logger.info(f"ğŸ¯ Variables cibles: {target_columns}")
            
            # SÃ©lection des features (exclure les cibles et colonnes non-numÃ©riques)
            feature_columns = Config.FEATURE_COLUMNS
            
            # Nettoyage final
            clean_df = df[target_columns + feature_columns].dropna()
            
            if len(clean_df) < MIN_DATA_POINTS:
                raise ValueError(f"DonnÃ©es insuffisantes aprÃ¨s nettoyage: {len(clean_df)}")
            
            # Division temporelle (80% train, 20% test)
            split_idx = int(len(clean_df) * split_ratio)
            train_df = clean_df.iloc[:split_idx]
            test_df = clean_df.iloc[split_idx:]
            
            # Matrices X et y
            X_train = train_df[feature_columns]
            y_train = train_df[target_columns]
            X_test = test_df[feature_columns]
            y_test = test_df[target_columns]
            
            # Normalisation
            if not self.is_fitted:
                X_train_scaled = self.scaler.fit_transform(X_train)
                self.is_fitted = True
                logger.info("âœ… Scaler ajustÃ© sur les donnÃ©es d'entraÃ®nement")
            else:
                X_train_scaled = self.scaler.transform(X_train)
                
            X_test_scaled = self.scaler.transform(X_test)
            
            # Sauvegarde des matrices dans data/features/
            np.save(self.features_path / "X_train.npy", X_train_scaled)
            np.save(self.features_path / "X_test.npy", X_test_scaled)
            np.save(self.features_path / "y_train.npy", y_train.values)
            np.save(self.features_path / "y_test.npy", y_test.values)
            logger.info(f"ğŸ’¾ Matrices ML sauvegardÃ©es dans: {self.features_path}")
            
            self._feature_columns = feature_columns
            self._target_columns = target_columns
            
            logger.info(f"ğŸš‚ DonnÃ©es d'entraÃ®nement: {X_train_scaled.shape}")
            logger.info(f"ğŸ§ª DonnÃ©es de test: {X_test_scaled.shape}")
            
            return {
                'X_train': X_train_scaled,
                'X_test': X_test_scaled,
                'y_train': y_train.values,
                'y_test': y_test.values,
                'feature_names': feature_columns,
                'target_names': target_columns,
                'train_dates': train_df.index.values if 'datetime' in train_df else None,
                'test_dates': test_df.index.values if 'datetime' in test_df else None,
                'scaler': self.scaler
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la prÃ©paration ML: {e}")
            raise
    
    # ============================================================================
    # SAUVEGARDE ET CHARGEMENT DU PIPELINE
    # ============================================================================
    
    def save_pipeline(self, filepath: str = 'models/data_pipeline.joblib') -> None:
        """Sauvegarde du pipeline"""
        logger.info(f"ğŸ’¾ Sauvegarde du pipeline: {filepath}")
        
        try:
            Path(filepath).parent.mkdir(parents=True, exist_ok=True)
            
            pipeline_data = {
                'scaler': self.scaler,
                'is_fitted': self.is_fitted,
                'raw_path': str(self.raw_path),
                'processed_path': str(self.processed_path),
                'features_path': str(self.features_path),
                'feature_columns': self._feature_columns,
                'target_columns': self._target_columns,
                'label_encoders': self.label_encoders,
                'saved_at': datetime.now()
            }
            
            joblib.dump(pipeline_data, filepath)
            logger.info("âœ… Pipeline sauvegardÃ© avec succÃ¨s")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la sauvegarde: {e}")
            raise
    
    def load_pipeline(self, filepath: str = 'models/data_pipeline.joblib') -> None:
        """Chargement du pipeline"""
        logger.info(f"ğŸ“‚ Chargement du pipeline: {filepath}")
        
        try:
            if not Path(filepath).exists():
                raise FileNotFoundError(f"Fichier pipeline non trouvÃ©: {filepath}")
                
            pipeline_data = joblib.load(filepath)
            
            self.scaler = pipeline_data['scaler']
            self.is_fitted = pipeline_data['is_fitted']
            self._feature_columns = pipeline_data.get('feature_columns', [])
            self._target_columns = pipeline_data.get('target_columns', [])
            self.label_encoders = pipeline_data.get('label_encoders', {})
            
            saved_at = pipeline_data.get('saved_at', 'Unknown')
            logger.info(f"âœ… Pipeline chargÃ© (sauvegardÃ© le: {saved_at})")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement: {e}")
            raise
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform new data using the pipeline preprocessing and feature engineering"""
        logger.info("ğŸ”„ Transformation des nouvelles donnÃ©es...")
        
        try:
            # Preprocess
            df_processed = self.marrakech_loader.preprocess_weather_data(df)
            # Feature engineering
            df_features = self.marrakech_loader.create_weather_features(df_processed)
            
            logger.info(f"âœ… Transformation terminÃ©e: {df_features.shape}")
            return df_features
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la transformation: {e}")
            raise
            
    def load_and_prepare_data(self) -> pd.DataFrame:
        """
        Charge et prÃ©pare les donnÃ©es pour la validation (wrapper pour Airflow).
        ExÃ©cute les Ã©tapes 1 Ã  3 si nÃ©cessaire.
        """
        logger.info("ğŸ”„ Chargement et prÃ©paration des donnÃ©es pour validation...")
        try:
            # Tenter de charger les features existantes
            features_file = self.features_path / "weather_data_features.csv"
            if features_file.exists():
                logger.info(f"ğŸ“‚ Chargement des features existantes: {features_file}")
                return pd.read_csv(features_file)
            
            # Sinon, exÃ©cuter le pipeline jusqu'Ã  la crÃ©ation des features
            logger.info("âš ï¸ Features non trouvÃ©es, exÃ©cution du pipeline...")
            raw_data = self.step1_download_raw_data()
            processed_data = self.step2_preprocess_data(raw_data)
            features_data = self.step3_create_features(processed_data)
            
            return features_data
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la prÃ©paration des donnÃ©es: {e}")
            raise
    
    # ============================================================================
    # PIPELINE COMPLET
    # ============================================================================
    
    def run_full_pipeline(self, split_ratio: float = DEFAULT_SPLIT_RATIO) -> Dict[str, Any]:
        """ExÃ©cution complÃ¨te du pipeline en 3 Ã©tapes"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸš€ DÃ‰MARRAGE DU PIPELINE COMPLET (3 Ã‰TAPES)")
        logger.info("=" * 70 + "\n")
        
        try:
            # Ã‰TAPE 1: TÃ©lÃ©chargement depuis Kaggle â†’ data/raw/
            raw_data = self.step1_download_raw_data()
            
            # Ã‰TAPE 2: Preprocessing â†’ data/processed/
            processed_data = self.step2_preprocess_data(raw_data)
            
            # Ã‰TAPE 3: CrÃ©ation des features â†’ data/features/
            features_data = self.step3_create_features(processed_data)
            
            # Validation
            is_valid, errors = self.validate_data(features_data)
            if not is_valid:
                raise ValueError(f"Validation Ã©chouÃ©e: {errors}")
            
            # PrÃ©paration ML
            ml_data = self.prepare_ml_data(features_data, split_ratio)
            
            # Sauvegarde du pipeline
            self.save_pipeline()
            
            # Statistiques finales
            stats = {
                'raw_shape': raw_data.shape,
                'processed_shape': processed_data.shape,
                'features_shape': features_data.shape,
                'train_shape': ml_data['X_train'].shape,
                'test_shape': ml_data['X_test'].shape,
                'feature_count': len(ml_data['feature_names']),
                'target': ml_data['target_names'],
                'pipeline_saved': True,
                'raw_file': str(self.raw_path / "weather_data_raw.csv"),
                'processed_file': str(self.processed_path / "weather_data_processed.csv"),
                'features_file': str(self.features_path / "weather_data_features.csv")
            }
            
            logger.info("\n" + "=" * 70)
            logger.info("ğŸ‰ PIPELINE COMPLET TERMINÃ‰ AVEC SUCCÃˆS")
            logger.info("=" * 70)
            logger.info(f"ğŸ“Š Ã‰TAPE 1 (Raw):       {stats['raw_shape']}")
            logger.info(f"ğŸ“Š Ã‰TAPE 2 (Processed): {stats['processed_shape']}")
            logger.info(f"ğŸ“Š Ã‰TAPE 3 (Features):  {stats['features_shape']}")
            logger.info(f"ğŸ“‚ Fichiers crÃ©Ã©s:")
            logger.info(f"   - {stats['raw_file']}")
            logger.info(f"   - {stats['processed_file']}")
            logger.info(f"   - {stats['features_file']}")
            logger.info("=" * 70 + "\n")
            
            return {
                'ml_data': ml_data,
                'stats': stats,
                'raw_data': raw_data,
                'processed_data': processed_data,
                'features_data': features_data
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur dans le pipeline complet: {e}")
            raise

# Alias pour compatibilitÃ© avec l'ancien code
ClimateDataPipeline = WeatherDataPipeline
DataPipeline = WeatherDataPipeline

def main():
    """Fonction principale pour tester le pipeline"""
    try:
        pipeline = WeatherDataPipeline()
        results = pipeline.run_full_pipeline()
        
        stats = results['stats']
        
        print("\nğŸ“Š RÃ‰SUMÃ‰ DU PIPELINE MÃ‰TÃ‰O:")
        print(f"   ğŸ“¥ DonnÃ©es brutes: {stats['raw_shape']}")
        print(f"   ğŸ“Š DonnÃ©es prÃ©processÃ©es: {stats['processed_shape']}")
        print(f"   ğŸ“Š DonnÃ©es avec features: {stats['features_shape']}")
        print(f"   ğŸš‚ EntraÃ®nement: {stats['train_shape']}")
        print(f"   ğŸ§ª Test: {stats['test_shape']}")
        print(f"   ğŸ¯ Variables cibles: {stats['target']}")
        print(f"   ğŸ“Š Features: {stats['feature_count']}")
        print(f"   ğŸ’¾ Pipeline sauvegardÃ©: {'âœ…' if stats['pipeline_saved'] else 'âŒ'}")
        
    except Exception as e:
        logger.error(f"âŒ Erreur dans le pipeline: {e}")
        raise

if __name__ == "__main__":
    main()