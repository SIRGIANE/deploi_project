"""
üìä Module de comparaison de mod√®les
Compare les performances des nouveaux mod√®les avec ceux en production
"""

import pandas as pd
import numpy as np
import json
import logging
from pathlib import Path
from datetime import datetime
import mlflow
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import os

logger = logging.getLogger(__name__)

class ModelComparator:
    """Comparateur de performance entre mod√®les"""
    
    def __init__(self):
        self.models_dir = Path("models")
        self.reports_dir = Path("reports/model_comparison")
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration MLflow
        mlflow.set_tracking_uri("file:./mlruns")
        
    def load_production_model_info(self):
        """Chargement des informations du mod√®le en production"""
        try:
            # Recherche du mod√®le en production dans MLflow
            client = mlflow.tracking.MlflowClient()
            
            # Chercher le mod√®le avec le tag "production"
            production_models = []
            
            for experiment in client.list_experiments():
                runs = client.search_runs(
                    experiment_ids=[experiment.experiment_id],
                    filter_string="tags.stage = 'production'"
                )
                
                for run in runs:
                    production_models.append({
                        'run_id': run.info.run_id,
                        'experiment_id': run.info.experiment_id,
                        'metrics': run.data.metrics,
                        'tags': run.data.tags,
                        'start_time': run.info.start_time
                    })
            
            if not production_models:
                logger.warning("‚ö†Ô∏è Aucun mod√®le en production trouv√©")
                return None
            
            # Prendre le plus r√©cent
            latest_prod = max(production_models, key=lambda x: x['start_time'])
            
            logger.info(f"üìã Mod√®le en production trouv√©: {latest_prod['run_id']}")
            return latest_prod
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le production: {e}")
            return None
    
    def load_current_model_metrics(self):
        """Chargement des m√©triques du mod√®le actuel"""
        try:
            # Chargement depuis le fichier de m√©triques DVC
            metrics_files = [
                "metrics.json",
                "evaluation_metrics.json"
            ]
            
            current_metrics = {}
            
            for metrics_file in metrics_files:
                if Path(metrics_file).exists():
                    with open(metrics_file, 'r') as f:
                        file_metrics = json.load(f)
                        current_metrics.update(file_metrics)
            
            # Recherche du run MLflow le plus r√©cent
            client = mlflow.tracking.MlflowClient()
            
            # Obtenir l'exp√©rience par d√©faut
            experiment = client.get_experiment_by_name("Default")
            if experiment is None:
                experiments = client.list_experiments()
                if experiments:
                    experiment = experiments[0]
            
            if experiment:
                runs = client.search_runs(
                    experiment_ids=[experiment.experiment_id],
                    order_by=["start_time DESC"],
                    max_results=1
                )
                
                if runs:
                    latest_run = runs[0]
                    mlflow_metrics = latest_run.data.metrics
                    current_metrics.update(mlflow_metrics)
                    
                    current_model_info = {
                        'run_id': latest_run.info.run_id,
                        'metrics': current_metrics,
                        'tags': latest_run.data.tags,
                        'start_time': latest_run.info.start_time
                    }
                    
                    logger.info(f"üìä M√©triques mod√®le actuel charg√©es: {len(current_metrics)} m√©triques")
                    return current_model_info
            
            # Si pas de MLflow, utiliser seulement les fichiers locaux
            if current_metrics:
                return {
                    'run_id': 'local_run',
                    'metrics': current_metrics,
                    'tags': {},
                    'start_time': datetime.now().timestamp() * 1000
                }
            
            logger.warning("‚ö†Ô∏è Aucune m√©trique actuelle trouv√©e")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement m√©triques actuelles: {e}")
            return None
    
    def evaluate_all_models(self):
        """√âvaluation de tous les mod√®les disponibles"""
        try:
            logger.info("üìä √âvaluation de tous les mod√®les...")
            
            # Chargement des donn√©es de test
            test_features_path = "data/features/X_test.npy"
            test_targets_path = "data/features/y_test.npy"
            
            if not (Path(test_features_path).exists() and Path(test_targets_path).exists()):
                logger.error("‚ùå Donn√©es de test manquantes")
                return {}
            
            X_test = np.load(test_features_path)
            y_test = np.load(test_targets_path)
            
            model_results = {}
            
            # √âvaluation des mod√®les disponibles
            model_files = {
                'random_forest': 'models/rf_model.pkl',
                'lstm': 'models/lstm_model.h5',
                'scaler': 'models/scaler.pkl'
            }
            
            # Chargement du scaler
            # Note: X_test.npy contient d√©j√† des donn√©es scal√©es par le pipeline
            # On ne doit pas les rescaler ici
            X_test_scaled = X_test
            
            # √âvaluation Random Forest
            if Path(model_files['random_forest']).exists():
                try:
                    rf_model = joblib.load(model_files['random_forest'])
                    rf_predictions = rf_model.predict(X_test_scaled)
                    
                    rf_metrics = self.calculate_metrics(y_test, rf_predictions, 'random_forest')
                    model_results['random_forest'] = rf_metrics
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur √©valuation Random Forest: {e}")
            
            # √âvaluation LSTM (si TensorFlow/Keras disponible)
            if Path(model_files['lstm']).exists():
                try:
                    import tensorflow as tf
                    from tensorflow import keras
                    
                    lstm_model = keras.models.load_model(model_files['lstm'])
                    
                    # Reshape pour LSTM si n√©cessaire
                    if len(X_test_scaled.shape) == 2:
                        X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])
                    else:
                        X_test_lstm = X_test_scaled
                    
                    lstm_predictions = lstm_model.predict(X_test_lstm).flatten()
                    
                    lstm_metrics = self.calculate_metrics(y_test, lstm_predictions, 'lstm')
                    model_results['lstm'] = lstm_metrics
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur √©valuation LSTM: {e}")
            
            logger.info(f"‚úÖ {len(model_results)} mod√®les √©valu√©s")
            return model_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur √©valuation mod√®les: {e}")
            return {}
    
    def calculate_metrics(self, y_true, y_pred, model_name):
        """Calcul des m√©triques de performance"""
        try:
            metrics = {
                'model_name': model_name,
                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
                'mae': mean_absolute_error(y_true, y_pred),
                'r2': r2_score(y_true, y_pred),
                'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,
                'predictions_count': len(y_pred),
                'evaluation_timestamp': datetime.now().isoformat()
            }
            
            # M√©triques additionnelles
            residuals = y_true - y_pred
            metrics.update({
                'mean_residual': np.mean(residuals),
                'std_residual': np.std(residuals),
                'max_error': np.max(np.abs(residuals)),
                'median_error': np.median(np.abs(residuals))
            })
            
            return metrics
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul m√©triques {model_name}: {e}")
            return {'model_name': model_name, 'error': str(e)}
    
    def compare_with_production(self, current_evaluations):
        """Comparaison avec le mod√®le en production"""
        try:
            logger.info("üîÑ Comparaison avec le mod√®le en production...")
            
            production_model = self.load_production_model_info()
            
            if not production_model:
                # Pas de mod√®le en production, promouvoir le meilleur actuel
                best_current = self.select_best_model(current_evaluations)
                
                return {
                    'new_model_better': True,
                    'best_model': best_current,
                    'production_model': None,
                    'comparison_reason': 'no_production_model',
                    'recommendation': 'deploy_best_current'
                }
            
            # Comparaison des m√©triques
            production_metrics = production_model['metrics']
            best_current = self.select_best_model(current_evaluations)
            
            if not best_current:
                return {
                    'new_model_better': False,
                    'best_model': None,
                    'production_model': production_model,
                    'comparison_reason': 'no_valid_current_model',
                    'recommendation': 'keep_production'
                }
            
            # Crit√®res de comparaison (RMSE principal)
            improvement_threshold = 0.05  # 5% d'am√©lioration minimum
            
            prod_rmse = production_metrics.get('rmse', float('inf'))
            current_rmse = best_current.get('rmse', float('inf'))
            
            improvement = (prod_rmse - current_rmse) / prod_rmse if prod_rmse > 0 else 0
            
            # Crit√®res additionnels
            criteria_met = 0
            total_criteria = 3
            
            # 1. RMSE am√©lioration
            if improvement >= improvement_threshold:
                criteria_met += 1
            
            # 2. R¬≤ am√©lioration
            prod_r2 = production_metrics.get('r2', 0)
            current_r2 = best_current.get('r2', 0)
            if current_r2 > prod_r2 * 1.02:  # 2% d'am√©lioration
                criteria_met += 1
            
            # 3. MAE am√©lioration
            prod_mae = production_metrics.get('mae', float('inf'))
            current_mae = best_current.get('mae', float('inf'))
            if current_mae < prod_mae * 0.98:  # 2% d'am√©lioration
                criteria_met += 1
            
            should_promote = criteria_met >= 2  # Au moins 2 crit√®res sur 3
            
            comparison_results = {
                'new_model_better': should_promote,
                'best_model': best_current,
                'production_model': production_model,
                'improvement_percentage': improvement * 100,
                'criteria_met': f"{criteria_met}/{total_criteria}",
                'detailed_comparison': {
                    'rmse': {'production': prod_rmse, 'current': current_rmse, 'improvement': improvement},
                    'r2': {'production': prod_r2, 'current': current_r2},
                    'mae': {'production': prod_mae, 'current': current_mae}
                },
                'recommendation': 'promote' if should_promote else 'keep_production'
            }
            
            # Sauvegarde du rapport de comparaison
            self.save_comparison_report(comparison_results)
            
            logger.info(f"üèÜ Nouveau mod√®le {'recommand√©' if should_promote else 'pas recommand√©'} "
                       f"(am√©lioration: {improvement*100:.2f}%)")
            
            return comparison_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur comparaison avec production: {e}")
            return {
                'new_model_better': False,
                'error': str(e),
                'recommendation': 'keep_production'
            }
    
    def select_best_model(self, evaluations):
        """S√©lection du meilleur mod√®le bas√© sur les m√©triques"""
        if not evaluations:
            return None
        
        # Crit√®re principal: RMSE le plus bas
        best_model = None
        best_rmse = float('inf')
        
        for model_name, metrics in evaluations.items():
            if 'error' in metrics:
                continue
                
            model_rmse = metrics.get('rmse', float('inf'))
            
            if model_rmse < best_rmse:
                best_rmse = model_rmse
                best_model = metrics.copy()
                best_model['name'] = model_name
                best_model['path'] = f"models/{model_name}_model.pkl"
        
        return best_model
    
    def save_comparison_report(self, results):
        """Sauvegarde du rapport de comparaison"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_path = self.reports_dir / f"model_comparison_{timestamp}.json"
            
            with open(report_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            logger.info(f"üíæ Rapport de comparaison sauvegard√©: {report_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde rapport comparaison: {e}")

# Fonctions pour Airflow
def evaluate_all_models():
    """Fonction d'√©valuation appel√©e par Airflow"""
    comparator = ModelComparator()
    return comparator.evaluate_all_models()

def compare_with_production(evaluation_results):
    """Fonction de comparaison appel√©e par Airflow"""
    comparator = ModelComparator()
    return comparator.compare_with_production(evaluation_results)

if __name__ == "__main__":
    # Test local
    logging.basicConfig(level=logging.INFO)
    
    comparator = ModelComparator()
    
    # √âvaluation
    evaluations = comparator.evaluate_all_models()
    print(f"üìä √âvaluations: {list(evaluations.keys())}")
    
    # Comparaison
    comparison = comparator.compare_with_production(evaluations)
    print(f"üèÜ Recommandation: {comparison['recommendation']}")